import face_recognition
from sklearn.cluster import DBSCAN
import numpy as np
import os
import shutil
from collections import Counter, defaultdict
import cv2
import subprocess
import sys
import config as cfg

def extract_face_data_from_video(video_path):
    """ë™ì˜ìƒì—ì„œ í”„ë ˆì„ì„ ìƒ˜í”Œë§í•˜ì—¬ ì–¼êµ´ ì¸ì½”ë”©, íƒ€ì„ìŠ¤íƒ¬í”„, ìœ„ì¹˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    print(f"ğŸ“¹ ë™ì˜ìƒ íŒŒì¼ '{video_path}' ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    video = cv2.VideoCapture(video_path)
    if not video.isOpened():
        print(f"ì˜¤ë¥˜: ë™ì˜ìƒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return None

    fps = video.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps / cfg.FRAME_SAMPLING_RATE)
    
    face_data = []
    frame_count = 0
    
    while video.isOpened():
        ret, frame = video.read()
        if not ret:
            break

        # ì§€ì •ëœ ê°„ê²©ì˜ í”„ë ˆì„ë§Œ ì²˜ë¦¬
        if frame_count % frame_interval == 0:
            timestamp = frame_count / fps
            
            # BGR (OpenCV) -> RGB (face_recognition)
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # ì–¼êµ´ ìœ„ì¹˜ íƒì§€ ë° ì¸ì½”ë”©
            face_locations = face_recognition.face_locations(rgb_frame)
            if face_locations:
                face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
                
                for i, encoding in enumerate(face_encodings):
                    face_data.append({
                        "encoding": encoding,
                        "timestamp": timestamp,
                        "location": face_locations[i] # ì–¼êµ´ ìœ„ì¹˜ ì €ì¥
                    })
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            sys.stdout.write(f"    - ì²˜ë¦¬ ì¤‘: {int(timestamp)}ì´ˆ ì§€ì , ì°¾ì€ ì–¼êµ´ ìˆ˜: {len(face_data)}ê°œ\r")
            sys.stdout.flush()

        frame_count += 1
        
    video.release()
    print(f"\nâœ¨ ì´ {len(face_data)}ê°œì˜ ì–¼êµ´ ë°ì´í„°ë¥¼ ë™ì˜ìƒì—ì„œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    return face_data


def save_representative_faces(video_path, face_data, labels):
    """ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    print("ğŸ–¼ï¸  ê° ê·¸ë£¹ì˜ ëŒ€í‘œ ì–¼êµ´ì„ ì €ì¥í•©ë‹ˆë‹¤...")
    
    # 1. ê° í´ëŸ¬ìŠ¤í„°(label)ì˜ ì²« ë²ˆì§¸ ì–¼êµ´ ë°ì´í„° ì°¾ê¸°
    first_faces = {}
    for i, label in enumerate(labels):
        if label != -1 and label not in first_faces:
            first_faces[label] = face_data[i]

    if not first_faces:
        print("    - ëŒ€í‘œ ì–¼êµ´ë¡œ ì§€ì •í•  í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. íƒ€ì„ìŠ¤íƒ¬í”„ë³„ë¡œ ì •ë¦¬í•˜ì—¬ ë¹„ë””ì˜¤ë¥¼ í•œ ë²ˆë§Œ íƒìƒ‰í•˜ë„ë¡ ìµœì í™”
    faces_by_timestamp = defaultdict(list)
    for label, data in first_faces.items():
        faces_by_timestamp[data['timestamp']].append({'label': label, 'location': data['location']})

    # 3. ë¹„ë””ì˜¤ë¥¼ ì—´ê³  í”„ë ˆì„ì„ íƒìƒ‰í•˜ë©° ì–¼êµ´ ì´ë¯¸ì§€ ì €ì¥
    video = cv2.VideoCapture(video_path)
    fps = video.get(cv2.CAP_PROP_FPS)

    for timestamp in sorted(faces_by_timestamp.keys()):
        frame_num = int(timestamp * fps)
        video.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
        ret, frame = video.read()

        if ret:
            for face_info in faces_by_timestamp[timestamp]:
                label = face_info['label']
                top, right, bottom, left = face_info['location']
                
                face_image = frame[top:bottom, left:right]
                
                person_dir = os.path.join(cfg.OUTPUT_CLIPS_DIR, f"person_{label}")
                os.makedirs(person_dir, exist_ok=True)
                
                output_path = os.path.join(person_dir, "representative_face.jpg")
                cv2.imwrite(output_path, face_image)
                print(f"    - 'person_{label}' ê·¸ë£¹ì˜ ëŒ€í‘œ ì–¼êµ´ ì €ì¥ ì™„ë£Œ.")

    video.release()


def generate_timelines(face_data, labels):
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ë¬¼ë³„ ë“±ì¥ íƒ€ì„ë¼ì¸ì„ ìƒì„±í•˜ê³  ë³‘í•©í•©ë‹ˆë‹¤."""
    print("â³ ì¸ë¬¼ë³„ ë“±ì¥ íƒ€ì„ë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    
    timelines = defaultdict(list)
    for i, data in enumerate(face_data):
        label = labels[i]
        if label != -1:
            timelines[label].append(data['timestamp'])

    merged_timelines = defaultdict(list)
    for label, timestamps in timelines.items():
        timestamps.sort()
        
        if not timestamps:
            continue
            
        current_start = timestamps[0]
        current_end = timestamps[0]
        
        for i in range(1, len(timestamps)):
            if timestamps[i] - current_end <= cfg.CLIP_MERGE_TOLERANCE_SEC:
                current_end = timestamps[i]
            else:
                if current_end - current_start > 1.0:
                    merged_timelines[label].append((current_start, current_end))
                current_start = timestamps[i]
                current_end = timestamps[i]
        
        if current_end - current_start > 1.0:
             merged_timelines[label].append((current_start, current_end))

    print("âœ… íƒ€ì„ë¼ì¸ ìƒì„± ì™„ë£Œ!")
    return merged_timelines


def create_clips_from_timelines(video_path, timelines):
    """ìƒì„±ëœ íƒ€ì„ë¼ì¸ì— ë”°ë¼ FFmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì˜ìƒ í´ë¦½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("ğŸ¬ ë™ì˜ìƒ í´ë¦½ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    for label, intervals in timelines.items():
        person_dir = os.path.join(cfg.OUTPUT_CLIPS_DIR, f"person_{label}")
        os.makedirs(person_dir, exist_ok=True) # ë””ë ‰í† ë¦¬ëŠ” ì´ë¯¸ ëŒ€í‘œ ì–¼êµ´ ì €ì¥ ì‹œ ìƒì„±ë˜ì—ˆì„ ìˆ˜ ìˆìŒ
        print(f"  - 'person_{label}' ê·¸ë£¹ì˜ í´ë¦½ ìƒì„± ì¤‘...")
        
        for i, (start, end) in enumerate(intervals):
            output_filename = os.path.join(person_dir, f"clip_{i+1}.mp4")
            
            cmd = [
                'ffmpeg', '-i', video_path,
                '-ss', str(start),
                '-to', str(end),
                '-c:v', 'libx264',
                '-c:a', 'aac',
                '-y',
                '-loglevel', 'error',
                output_filename
            ]
            
            try:
                subprocess.run(cmd, check=True)
            except FileNotFoundError:
                print("\nâŒì¹˜ëª…ì  ì˜¤ë¥˜: 'ffmpeg'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì— FFmpegì´ ì„¤ì¹˜ë˜ì–´ ìˆê³ , PATHì— ë“±ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return
            except subprocess.CalledProcessError as e:
                print(f"\nâš ï¸ FFmpeg ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'output_clips' í´ë”ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")


def process_video(video_path):
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸: ë™ì˜ìƒ ì²˜ë¦¬, í´ëŸ¬ìŠ¤í„°ë§, í´ë¦½ ìƒì„±ì„ ì´ê´„í•©ë‹ˆë‹¤."""
    
    face_data = extract_face_data_from_video(video_path)
    if not face_data or len(face_data) < cfg.DBSCAN_MIN_SAMPLES:
        print("ğŸ¤·â€â™€ï¸ ì–¼êµ´ì„ ì¶©ë¶„íˆ ì°¾ì§€ ëª»í•´ í´ëŸ¬ìŠ¤í„°ë§ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    encodings = [d['encoding'] for d in face_data]
    print(f"\nğŸ¤– ì´ {len(encodings)}ê°œì˜ ì–¼êµ´ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    clt = DBSCAN(metric="euclidean", eps=cfg.DBSCAN_EPS, min_samples=cfg.DBSCAN_MIN_SAMPLES)
    clt.fit(encodings)
    
    labels = clt.labels_
    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    noise_count = np.sum(np.array(labels) == -1)
    print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ! ì´ {num_clusters}ê°œì˜ ì¸ë¬¼ ê·¸ë£¹ê³¼ {noise_count}ê°œì˜ ë¶„ë¥˜ë˜ì§€ ì•Šì€ ì–¼êµ´ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # ì¶œë ¥ í´ë” ì´ˆê¸°í™”
    if os.path.exists(cfg.OUTPUT_CLIPS_DIR):
        shutil.rmtree(cfg.OUTPUT_CLIPS_DIR)
    os.makedirs(cfg.OUTPUT_CLIPS_DIR)

    # ëŒ€í‘œ ì–¼êµ´ ì €ì¥
    save_representative_faces(video_path, face_data, labels)

    # íƒ€ì„ë¼ì¸ ìƒì„±
    timelines = generate_timelines(face_data, labels)
    if not timelines:
        print("ğŸ¤·â€â™‚ï¸ í´ë¦½ì„ ìƒì„±í•  ìœ ì˜ë¯¸í•œ ë“±ì¥ êµ¬ê°„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # í´ë¦½ ìƒì„±
    create_clips_from_timelines(video_path, timelines)


if __name__ == "__main__":
    if not os.path.exists(cfg.INPUT_VIDEO_DIR):
        os.makedirs(cfg.INPUT_VIDEO_DIR)
        print(f"í´ë” ìƒì„±: '{cfg.INPUT_VIDEO_DIR}'. ì´ í´ë”ì— ë¶„ì„í•  ë™ì˜ìƒì„ ë„£ì–´ì£¼ì„¸ìš”.")
        sys.exit()

    video_files = [f for f in os.listdir(cfg.INPUT_VIDEO_DIR) if f.lower().endswith(('.mp4', '.mov', '.avi', '.mkv'))]
    
    if not video_files:
        print(f"ë¶„ì„í•  ë™ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤. '{cfg.INPUT_VIDEO_DIR}' í´ë”ì— ë™ì˜ìƒ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        sys.exit()

    video_to_process = os.path.join(cfg.INPUT_VIDEO_DIR, video_files[0])
    
    process_video(video_to_process)